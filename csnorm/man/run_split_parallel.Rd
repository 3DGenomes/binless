% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/normalize.R
\name{run_split_parallel}
\alias{run_split_parallel}
\title{Cut-site normalization (parallelized)}
\usage{
run_split_parallel(counts, biases, square.size = 1e+05, coverage = 4,
  coverage.extradiag = 1, bf_per_kb = 1, bf_per_decade = 5,
  distance_bins_per_decade = 100, verbose = F, iter = 1e+05, ncpus = 30,
  homogenize = F, outprefix = NULL, circularize = -1L, ops.bias = NULL,
  ops.count = NULL)
}
\arguments{
\item{counts, }{biases data.tables as returned by 
\code{\link{prepare_for_sparse_cs_norm}}}

\item{square.size}{positive integer. Size of the subset of data (in base 
pairs) to normalize independently. If too large, optimization fails to 
converge and takes too long. If too small, estimates are heavily biased.}

\item{coverage}{positive integer. To reduce border effects, squares overlap 
and estimates are computed using a weighted average that favors the center 
of the square. Coverage indicates how many squares cover any portion of the
genome. This parameter is for the estimation of the genomic biases.}

\item{coverage.extradiag}{positive integer. Same as previous, but in the 
decay step.}

\item{bf_per_kb}{positive numeric. Number of cubic spline basis functions per
kilobase, for genomic bias estimates. Small values make the optimization 
easy, but makes the genomic biases stiffer.}

\item{bf_per_decade}{positive numeric. Number of cubic spline basis functions
per distance decade (in bases), for diagonal decay. Default parameter 
should suffice.}

\item{distance_bins_per_decade}{positive integer. How many bins per decade 
should be used to discretize and re-fit the diagonal decay. Default 
parameter should suffice. Must be much larger than bf_per_decade: ideally
in that bin any basis function should be approximately constant.}

\item{verbose}{boolean. Show output of different steps.}

\item{iter}{positive integer. Number of optimization steps for each stan
model call.}

\item{ncpus}{positive integer. Number of CPUs to parallelize on.}

\item{homogenize}{boolean. Should the biases be homogenized for stiffness?
Default is FALSE}

\item{outprefix}{character. If not NULL, prefix used to write intermediate
output files. Diagnostics only.}

\item{circularize}{integer. Set this to the size of the chromosome if it is 
circular, otherwise leave as-is (default is -1)}

\item{ops.bias, }{ops.count if not NULL, skip the genomic (resp. decay) bias
estimation step, and use these intermediate data tables instead.}
}
\value{
A list containing: par: the optimized parameters. out.bias: the last
  line of the stan output for the bias estimation. runtime.bias: the runtimes
  of these optimizations. out.count: the last line of the stan output for the
  decay estimation. runtime.count: the runtimes of these
  optimizations.
}
\description{
The dataset is splitted into subsets which are run in parallel and they are 
then stitched together. First, genomic biases nu and delta are estimated by 
cutting the genome in overlapping segments. Mean estimates are then computed 
for nu and delta. These estimates can then be homogenized to have a constant 
stiffness. Second, squares covering all the dataset are extracted in order to
compute the diagonal decay given nu and delta. Again, estimates are computed 
for the diagonal decay. Finally, the count exposure is recomputed for the 
final estimates of nu, delta and decay.
}

